{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models_validation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORD4TWPkit6ij99wWkdX74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiaramarzi/ML-models-validation/blob/main/models_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTe8Jj3qhlw5"
      },
      "source": [
        "# Artificial intelligence (AI) for health - potentials\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGoOZvGALKgw"
      },
      "source": [
        "*   **Data mining**: finding pattern in big data\n",
        "*   **Biomarker discovery**: determining potential (compound) biomarkers\n",
        "*   The **predicitive nature** of machine learning strategies is highly in line with the aim of clinical diagnosis and prognosis **in the single patient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUGpx9Z7iqL0"
      },
      "source": [
        "# Models validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JwXyfF-LRej"
      },
      "source": [
        "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.\n",
        "Model validation is carried out after model training.\n",
        "\n",
        "Estimation of **unbiased generalization performance** of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_Jo9T8SBbc"
      },
      "source": [
        "# Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9T0es-dSFsQ"
      },
      "source": [
        "* Holdout validation\n",
        "* K-fold cross-validation (CV)\n",
        "* Leave-One-Out CV (LOOCV)\n",
        "* Hyperparameters tuning\n",
        "* Training, validation and test set: the holdout validation\n",
        "* Training, validation and test set: the nested CV\n",
        "* Sampling bias\n",
        "* Repetition of holdout validation\n",
        "* Repetition of CV\n",
        "* Unbalanced datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfjOSjyakq9H"
      },
      "source": [
        "# Age prediction based on neuroimaging features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTl_nz8OLv_C"
      },
      "source": [
        "\n",
        "\n",
        "*   Data: T1-weighted images of 86 healthy subjects with age ranging from 19 to 85 years (41 males and 45 females, age 44.2 ± 17.1 years, mean ± standard deviation). Data are freely accessible at [here](https://fcon_1000.projects.nitrc.org/) and described in (Mazziotta et al., 2001)\n",
        "*   Features:\n",
        "  * Cortical thickness (mCT)\n",
        "  * Gyrification index (Pial_mean_GI)\n",
        "  * Fractal dimension (FD)\n",
        "* Task:\n",
        "  * Regression:\n",
        "    * Estimator: Support Vector Machines (SVR)\n",
        "    * Performance: Mean Absolute Error (MAE)\n",
        "  * Classification (\"young\" vs. \"old\"):\n",
        "    * Estimator: Support Vector Machines (SVC)\n",
        "    * Performance: Accuracy\n",
        "\n",
        "The same data and features have been previously investigated in (Marzi et al., 2020).\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "Marzi, C., Giannelli, M., Tessa, C. et al. Toward a more reliable characterization of fractal properties of the cerebral cortex of healthy subjects during the lifespan. Sci Rep 10, 16957 (2020). https://doi.org/10.1038/s41598-020-73961-w\n",
        "\n",
        "Mazziotta, J. et al. A probabilistic atlas and reference system for the human brain: International Consortium for Brain Mapping (ICBM). Philos. Trans. R. Soc. Lond. B Biol. Sci. 356, 1293–1322. https://doi.org/10.1098/rstb.2001.0915 (2001)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im3BWk_5OVtS"
      },
      "source": [
        "# Cloning repository, libraries and data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MZY9faAOjaj"
      },
      "source": [
        "# My repo cloning\n",
        "! git clone https://github.com/chiaramarzi/ML-models-validation\n",
        "\n",
        "%cd /content/ML-models-validation\n",
        "! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmE6GnKUpQ7"
      },
      "source": [
        "# Libraries loading\n",
        "from IPython.display import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_validate\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
        "\n",
        "%run utils.ipynb import *\n",
        "\n",
        "# Regression data\n",
        "reg_data = pd.read_csv('data_regression.csv')\n",
        "\n",
        "# Balanced classification data\n",
        "class_data = pd.read_csv('data_classification_balanced.csv')\n",
        "\n",
        "# Unbalanced classification data\n",
        "unbal_class_data = pd.read_csv('data_classification_unbalanced.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSE0z7esU8Ky"
      },
      "source": [
        "reg_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndM8efnpVGAP"
      },
      "source": [
        "class_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YHx6EyaVHwK"
      },
      "source": [
        "unbal_class_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6G4jorBVPOI"
      },
      "source": [
        "# Holdout validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZRcU-8ZtQ0h"
      },
      "source": [
        "The principle is simple, you simply split your data randomly into roughly 70% used for training the model and 30% for testing the model. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moD3nhOSY0az"
      },
      "source": [
        "![](https://raw.githubusercontent.com/chiaramarzi/ML-models-validation/main/figures/IMG_4103.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z4UNX79uxKr"
      },
      "source": [
        "Image('figures/IMG_4103.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ftXYNS7hM81"
      },
      "source": [
        "#SEED = 42 #563: good, 0: perfect, 42: worse\n",
        "\n",
        "### REGRESSION ###\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "regression_holdout(X, y, seed = 563, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J2_qJfzk-xl"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "print('***Classification task')\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" (<= 30 years) subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" (>= 56 years) subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "classification_holdout(X, y, seed = 43, stratify = None, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBrm5c1DlgTV"
      },
      "source": [
        "# K-fold cross-validation (CV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DJkmJ3alik5"
      },
      "source": [
        "It splits the data into k folds, then trains the data on k-1 folds and test on the one fold that was left out. It does this for all combinations and averages the result on each instance.\n",
        "\n",
        "The advantage is that all observations are used for both training and validation, and each observation is used once for validation. \n",
        "\n",
        "We typically choose either k=5 or k=10 as they find a nice balance between computational complexity and validation accuracy.\n",
        "\n",
        "The scores of each fold from CV techniques are more insightful than one may think. They are mostly used to simply extract the average performance. However, one might also look at the variance or standard deviation of the resulting folds as it will give information about the stability of the model across different data inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHpBE5XxmCpJ"
      },
      "source": [
        "### REGRESSION ###\n",
        "n_folds = 5 # for LOOCV insert n_fold = 86\n",
        "\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "MAE_train, MAE_test = regression_CV(X, y, seed = 42, n_folds = n_folds)\n",
        "print_to_std(MAE_train, MAE_test, \"MAE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2aP-OsmhDK"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "n_folds = 5 # for LOOCV insert n_fold = 50\n",
        "\n",
        "print('***Classification task')\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "ACC_train, ACC_test = classification_CV(X, y, seed = 42, n_folds = n_folds)\n",
        "print_to_std(ACC_train, ACC_test, \"ACC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XT91xJqm3in"
      },
      "source": [
        "  # Leave-one-out CV (LOOCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgu3uRNBm-Jf"
      },
      "source": [
        "A variant of k-fold CV is Leave-one-out Cross-Validation (LOOCV). \n",
        "\n",
        "LOOCV uses each sample in the data as a separate test set while all remaining samples form the training set. This variant is identical to k-fold CV when k = n (number of observations).\n",
        "\n",
        "LOOCV is computationally very costly as the model needs to be trained n times. Only do this if the dataset is small or if you can handle that many computations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuoxXbBGnPi0"
      },
      "source": [
        "### REGRESSION ###\n",
        "n_folds = 86\n",
        "\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "MAE_train, MAE_test = regression_CV(X, y, seed = 42, n_folds = n_folds)\n",
        "print_to_std(MAE_train, MAE_test, \"MAE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBzy85wWncCP"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "n_folds = 50\n",
        "\n",
        "print('***Classification task')\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "ACC_train, ACC_test = classification_CV(X, y, seed = 42, n_folds = n_folds)\n",
        "print_to_std(ACC_train, ACC_test, \"ACC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-0dPq40nlXl"
      },
      "source": [
        "# Hyperparameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xN-9qQFnvl_"
      },
      "source": [
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "* Effective in high dimensional spaces.\n",
        "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "Both SVR() and SVC() classes have, among others, a **regularization parameter**. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. The strength of the regularization is inversely proportional to C. Must be strictly positive. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtagDWcOn6ur"
      },
      "source": [
        "# Training, validation and test set: the holdout validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y1riVe2oLTV"
      },
      "source": [
        "When optimizing the hyperparameters of your model, you might overfityour model if you were to optimize using the train/test split.\n",
        "Why? Because the model searches for the hyperparameters that fit the specific train/test you made.\n",
        "\n",
        "To solve this issue, you can create an additional holdout set. This is often 10% of the data which you have not used in any of your processing/validation steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLqXCvQhoVFP"
      },
      "source": [
        "SEED = 42 #563: good, 0: perfect, 42: worse\n",
        "C = [0.1, 1, 10, 100]\n",
        "\n",
        "### REGRESSION ###\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "regression_holdout_val_set(X, y, SEED, test_set_size = 0.25, val_set_size = 0.15, C=C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHnjLjXLqI1-"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "print('***Classification task')\n",
        "\n",
        "C = [0.1, 1, 10, 100]\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "classification_holdout_val_set(X, y, SEED, test_set_size = 0.25, val_set_size = 0.15, C=C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzqJx2cMqalD"
      },
      "source": [
        "# Training, validation and test set: the nested CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaADFev8rolQ"
      },
      "source": [
        "When you are optimizing the hyperparameters of your model and you use the same k-Fold CV strategy to tune the model and evaluate performance you run the risk of overfitting. You do not want to estimate the accuracy of your model on the same split that you found the best hyperparameters for.\n",
        "\n",
        "\n",
        "Instead, we use a Nested Cross-Validation strategy allowing to separate the hyperparameter tuning step from the error estimation step. To do this, we nest two k-fold cross-validation loops:\n",
        "\n",
        "\n",
        "*   The inner loop for hyperparameter tuning and\n",
        "*   the outer loop for estimating accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-U58KpmsGlN"
      },
      "source": [
        "### REGRESSION ###\n",
        "print('***Regression task')\n",
        "\n",
        "SEED = 42\n",
        "outer_n_folds = 5\n",
        "inner_n_folds = 5\n",
        "C = [0.1, 1, 10]\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "MAE_tr_val, MAE_test = regression_nestedCV(X, y, SEED, outer_n_folds, inner_n_folds, C)\n",
        "print_to_std(MAE_tr_val, MAE_test, \"MAE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2xDS8rHz5Y"
      },
      "source": [
        "# NestedCV implemented in scikit-learn\n",
        "outer_cv = KFold(n_splits=outer_n_folds, shuffle=True, random_state=SEED)\n",
        "inner_cv = KFold(n_splits=inner_n_folds, shuffle=True, random_state=SEED)\n",
        "\n",
        "clf = SVR(kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=0.1, epsilon=0.1, shrinking=True, cache_size=200, verbose=0, max_iter=- 1)\n",
        "p_grid = [{'C': C}]     \n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "clf_gs = GridSearchCV(clf, param_grid=p_grid, cv=inner_cv, refit='neg_mean_absolute_error', scoring='neg_mean_absolute_error', n_jobs=1, verbose = 4)\n",
        "nested_score = cross_validate(clf_gs, X=X, y=y, cv=outer_cv, return_train_score=True, return_estimator=True, scoring = 'neg_mean_absolute_error', n_jobs=1)\n",
        "\n",
        "#print(np.abs(nested_score['train_score']))\n",
        "#print(np.abs(nested_score['test_score']))\n",
        "print(\"Average MAE train:\", np.abs(np.mean(nested_score['train_score'])))\n",
        "print(\"Average MAE test:\", np.abs(np.mean(nested_score['test_score'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA-GopPOsmo4"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "print('***Classification task')\n",
        "\n",
        "SEED = 42\n",
        "outer_n_folds = 5\n",
        "inner_n_folds = 5\n",
        "C = [0.1, 1, 10]\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "ACC_train, ACC_test = classification_nestedCV(X, y, SEED, outer_n_folds, inner_n_folds, C)\n",
        "print_to_std(ACC_train, ACC_test, \"ACC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEm4D3AVs1hX"
      },
      "source": [
        "# Sampling bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugdUBZ_Ys-A4"
      },
      "source": [
        "What if one subset of our data only have people of a certain age or income levels? This is typically referred to as a sampling bias. \n",
        "\n",
        "**Sampling bias** is systematic error due to a non-random sample of a population, causing some members of the population to be less likely to be included than others, resulting in a biased sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T1cTdbQtITQ"
      },
      "source": [
        "# Repetition of holdout validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATIUK0JItLdc"
      },
      "source": [
        "print('#### SAMPLING BIAS: HOLDOUT REPETITION')\n",
        "\n",
        "### REGRESSION ###\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "for SEED in range(0,10):\n",
        "  print(\"Seed:\", SEED)\n",
        "  regression_holdout(X, y, seed = SEED, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re_UEY8xte1a"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "print('***Classification task')\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "for SEED in range(0,10):\n",
        "  print(\"Seed:\", SEED)\n",
        "  classification_holdout(X, y, SEED, test_size = 0.25, stratify = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDKhgOEptvRx"
      },
      "source": [
        "# Repetition of CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd9vC_WFtw_6"
      },
      "source": [
        "print('#### SAMPLING BIAS: K-FOLD CV REPETITION')\n",
        "\n",
        "### REGRESSION ###\n",
        "n_folds = 5\n",
        "\n",
        "print('***Regression task')\n",
        "\n",
        "X = reg_data.iloc[:,2:5]\n",
        "y = reg_data['Age']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(reg_data)[0]) + ' subjects')\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "for SEED in range(1,10):\n",
        "  print(\"Seed:\", SEED)\n",
        "  MAE_train, MAE_test = regression_CV(X, y, SEED, n_folds)\n",
        "  print_to_std(MAE_train, MAE_test, \"MAE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byN32Bd6uCZo"
      },
      "source": [
        "### CLASSIFICATION ###\n",
        "n_folds = 5 \n",
        "\n",
        "print('***Classification task')\n",
        "\n",
        "X = class_data.iloc[:,2:5]\n",
        "y = class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "for SEED in range(1,10):\n",
        "  print(\"Seed:\", SEED)\n",
        "  ACC_train, ACC_test = classification_CV(X, y, SEED, n_folds)\n",
        "  print_to_std(ACC_train, ACC_test, \"ACC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQOYF10-uSR8"
      },
      "source": [
        "# Unbalanced datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlVF4wxeuX94"
      },
      "source": [
        "In some cases, there may be a large imbalance in the response variables. \n",
        "\n",
        "For example, in case of classification, there might be several times more negative samples than positive samples. For such problems, a slight variation in the K-Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. \n",
        "\n",
        "This variation is also known as **Stratified** K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAITJP9Wuj2Z"
      },
      "source": [
        "SEED = 88\n",
        "#### UNBALANCED DATASETS ###\n",
        "print('#### UNBALANCED DATASETS')\n",
        "### CLASSIFICATION ###\n",
        "print('***Classification task')\n",
        "\n",
        "X = unbal_class_data.iloc[:,2:5]\n",
        "y = unbal_class_data['Age_class']\n",
        "\n",
        "print('The whole dataset contains ' + str(np.shape(unbal_class_data)[0]) + ' subjects')\n",
        "print(\"\\\"young\\\" (<= 30 years) subjects in the sample:\", np.sum(y==0))\n",
        "print(\"\\\"old\\\" (> 30 years) subjects in the sample:\",np.sum(y==1))\n",
        "print()\n",
        "print('The age prediction will be performed using ' + str(np.shape(X)[1]) + ' MRI-derived features')\n",
        "print() \n",
        "\n",
        "'''\n",
        "for SEED in range(0,100):\n",
        "    print(\"SEED:\", SEED)\n",
        "    classification_holdout(X, y, SEED, stratify = None)\n",
        "    classification_holdout(X, y, SEED, stratify = y)\n",
        "\n",
        "'''\n",
        "print(\"Unbalanced dataset - unstratified holdout\")\n",
        "classification_holdout(X, y, SEED, test_size = 0.25, stratify = None)\n",
        "print(\"Unbalanced dataset - stratified holdout\")\n",
        "classification_holdout(X, y, SEED, test_size = 0.25, stratify = y)\n",
        "\n",
        "# SEED = 95, 91, 88"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}